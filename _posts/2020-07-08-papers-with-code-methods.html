---
title: Papers with code - Methods page
date: 2020-07-08
layout: post
tags: [math, deep-learning, machine-learning]
---

<a href="https://paperswithcode.com/methods">
	<img src="/px/math/papers-with-code-methods.png" width="98%"></a>
	<br>
	<br>

<h2>A list of methods (in progress)</h2>

<div style="column-count: 2;">

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/skip-connection-blocks">
			skip connection blocks</a></strong><br>
		<a href="https://paperswithcode.com/method/residual-block">residual</a> | 
		<a href="https://paperswithcode.com/method/bottleneck-residual-block">bottleneck residual</a> | 
		<a href="https://paperswithcode.com/method/dense-block">dense</a> | 
		<a href="https://paperswithcode.com/method/inverted-residual-block">residual (inverted)</a> | 
		<a href="https://paperswithcode.com/method/resnext-block">ResNeXt</a> | 
		<a href="https://paperswithcode.com/method/non-local-block">Non-Local</a> | 
		<a href="https://paperswithcode.com/method/wide-residual-block">residual (wide)</a> | 
		<a href="https://paperswithcode.com/method/shufflenet-block">shuffleNet</a> | 
		<a href="https://paperswithcode.com/method/cbhg">CBHG</a> | 
		<a href="https://paperswithcode.com/method/inception-resnet-v2-c">Inception-ResNet-v2-C</a> | 
		<a href="https://paperswithcode.com/method/inception-resnet-v2-b">Inception-ResNet-v2-B</a> | 
		<a href="https://paperswithcode.com/method/pyramidal-residual-unit">residual (pyramidal)</a>
	</div>

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/activation-functions">activation functions</a></strong><br>
		<a href="https://paperswithcode.com/method/relu">ReLU</a> | 
		<a href="https://paperswithcode.com/method/sigmoid-activation">Sigmoid</a> | 
		<a href="https://paperswithcode.com/method/tanh-activation">Tanh</a> | 
		<a href="https://paperswithcode.com/method/gelu">GeLU</a> |
		<a href="https://paperswithcode.com/method/leaky-relu">Leaky ReLU</a> |
		<a href="https://paperswithcode.com/method/swish">Swish</a> | 
		<a href="https://paperswithcode.com/method/maxout">Maxout</a> | 
		<a href="https://paperswithcode.com/method/prelu">PReLU</a> |
		<a href="https://paperswithcode.com/method/elu">ELU</a> | 
		<a href="https://paperswithcode.com/method/selu">SELU</a> |
		<a href="https://paperswithcode.com/method/softplus">Softplus</a>
	</div>

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/regularization">
			regularization</a></strong><br>
		<a href="https://paperswithcode.com/method/dropout">Dropout</a> |
		<a href="https://paperswithcode.com/method/weight-decay">Weight Decay</a> |
		<a href="https://paperswithcode.com/method/label-smoothing">Label Smoothing</a> |
		<a href="https://paperswithcode.com/method/attention-dropout">Attention Dropout</a> |
		<a href="https://paperswithcode.com/method/entropy-regularization">Entropy Regularization</a> |
		<a href="https://paperswithcode.com/method/early-stopping">Early Stopping</a> |
		<a href="https://paperswithcode.com/method/variational-dropout">Variational Dropout</a> |
		<a href="https://paperswithcode.com/method/dropconnect">Drop Connect</a> |
		<a href="https://paperswithcode.com/method/r1-regularization">R1</a> |
		<a href="https://paperswithcode.com/method/embedding-dropout">Embedding Dropout</a>
	</div>

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/stochastic-optimization">
			stochastic optimization</a></strong><br>
		<a href="https://paperswithcode.com/method/adam">Adam</a> |
		<a href="https://paperswithcode.com/method/sgd">SGD</a> |
		<a href="https://paperswithcode.com/method/rmsprop">RMS Prop</a> |
		<a href="https://paperswithcode.com/method/sgd-with-momentum">SGD-Momentum</a> |
		<a href="https://paperswithcode.com/method/adagrad">AdaGrad</a> |
		<a href="https://paperswithcode.com/method/lamb">Lamb</a> |
		<a href="https://paperswithcode.com/method/nesterov-accelerated-gradient">Nesterov</a> |
		<a href="https://paperswithcode.com/method/amsgrad">AMSGrad</a> |
		<a href="https://paperswithcode.com/method/adamw">AdamW</a> |
		<a href="https://paperswithcode.com/method/lars">LARS</a>
	</div>

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/attention-mechanisms">attention mechanisms</a></strong><br>
		<a href="https://paperswithcode.com/method/multi-head-attention">Multi-Head</a> | 
		<a href="https://paperswithcode.com/method/scaled">Scaled Dot-Product</a> | 
		<a href="https://paperswithcode.com/method/additive-attention">Additive</a> | 
		<a href="https://paperswithcode.com/method/dot-product-attention">Dot-Product</a> | 
		<a href="https://paperswithcode.com/method/sagan-attention-module">SAGAN (Self-Attention)</a> | 
		<a href="https://paperswithcode.com/method/location-based-attention">Location-based</a> | 
		<a href="https://paperswithcode.com/method/content-based-attention">Content-based</a> | 
		<a href="https://paperswithcode.com/method/spatial-attention-module">Spatial</a> | 
		<a href="https://paperswithcode.com/method/channel-attention-module">Channel</a> | 
		<a href="https://paperswithcode.com/method/spatial-attention-guided-mask">
			Spatial Attention-Guided</a> | 
		<a href="https://paperswithcode.com/method/location-sensitive-attention">
			Location-Sensitive</a>
	</div>

	<div class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/normalization">
			normalization</a></strong><br>
		<a href="https://paperswithcode.com/method/layer-normalization">Layers</a> | 
		<a href="https://paperswithcode.com/method/batch-normalization">Batches</a> | 
		<a href="https://paperswithcode.com/method/local-response-normalization">Local Response</a> | 
		<a href="https://paperswithcode.com/method/instance-normalization">Instance</a> | 
		<a href="https://paperswithcode.com/method/spectral-normalization">Spectral</a> | 
		<a href="https://paperswithcode.com/method/adaptive-instance-normalization">Adaptive Instance</a> | 
		<a href="https://paperswithcode.com/method/weight-normalization">Weights</a> | 
		<a href="https://paperswithcode.com/method/conditional-batch-normalization">Batches (Conditional)</a> | 
		<a href="https://paperswithcode.com/method/group-normalization">Groups</a> | 
		<a href="https://paperswithcode.com/method/activation-normalization">Activations</a> | 
		<a href="https://paperswithcode.com/method/weight-demodulation">Weight Demodulation</a> | 
		<a href="https://paperswithcode.com/method/switchable-normalization">Switchable</a>
	</div>

	<div  class="textcard">
		<strong><a href="https://paperswithcode.com/methods/category/loss-functions">
			loss functions</a></strong><br>
		<a href="https://paperswithcode.com/method/cycle-consistency-loss">cycle consistency</a> | 
		<a href="https://paperswithcode.com/method/gan-least-squares-loss">GAN least squares</a> | 
		<a href="https://paperswithcode.com/method/focal-loss">focal</a> | 
		<a href="https://paperswithcode.com/method/gan-hinge-loss">GAN hinge</a> | 
		<a href="https://paperswithcode.com/method/infonce">infoNCE</a> | 
		<a href="https://paperswithcode.com/method/wgan-gp-loss">WGAN-GP</a> | 
		<a href="https://paperswithcode.com/method/ctc-loss">CTC</a> | 
		<a href="https://paperswithcode.com/method/nt-xent">NT-Xent</a> | 
		<a href="https://paperswithcode.com/method/lovasz-softmax">Lovasz-softmax</a> | 
		<a href="https://paperswithcode.com/method/balanced-l1-loss">balanced L1</a> | 
		<a href="https://paperswithcode.com/method/supervised-contrastive-loss">supervised contrastive</a> | 
		<a href="https://paperswithcode.com/method/dynamic-smoothl1-loss">dynamic smooth L1</a>
	</div>

<div class="textcard">
	<strong><a href="https://paperswithcode.com/methods/category/learning-rate-schedules">learning (rate schedules)</a></strong><br>
	<a href="https://paperswithcode.com/method/linear-warmup-with-linear-decay">
		linear warmup, linear decay</a> | 
	<a href="https://paperswithcode.com/method/linear-warmup-with-cosine-annealing">
		linear warmup, cosine annealing</a> | 
	<a href="https://paperswithcode.com/method/step-decay">step decay</a> | 
	<a href="https://paperswithcode.com/method/exponential-decay">exponential decay</a> | 
	<a href="https://paperswithcode.com/method/cosine-annealing">cosine annealing</a> | 
	<a href="https://paperswithcode.com/method/slanted-triangular-learning-rates">slanted triangular</a> | 
	<a href="https://paperswithcode.com/method/polynomial-rate-decay">polynomial rate decay</a> | 
	<a href="https://paperswithcode.com/method/linear-warmup">linear warmup</a> | 
	<a href="https://paperswithcode.com/method/inverse-square-root-schedule">inverse square root</a> | 
	<a href="https://paperswithcode.com/method/cyclical-learning-rate-policy">
		cyclical policy</a>
</div>

<div class="textcard">
	<strong><a href="https://paperswithcode.com/methods/category/neural-architecture-search">
		neural architecture search (NAS)</a></strong><br>
	<a href="https://paperswithcode.com/method/proxylessnas">proxyless</a> | 
	<a href="https://paperswithcode.com/method/detnas">Det</a> | 
	<a href="https://paperswithcode.com/method/pnas">progressive</a> | 
	<a href="https://paperswithcode.com/method/dnas">differentiable</a> | 
	<a href="https://paperswithcode.com/method/aging-evolution">aging evolution</a> | 
	<a href="https://paperswithcode.com/method/neural-architecture-search">NAS</a> | 
	<a href="https://paperswithcode.com/method/scarlet-nas">scarlet</a> | 
	<a href="https://paperswithcode.com/method/greedynas">greedy</a> | 
	<a href="https://paperswithcode.com/method/densenas">dense</a> | 
	<a href="https://paperswithcode.com/method/hit-detector">hit detector</a>
</div>

</div>

<!--
<a href="https://paperswithcode.com/methods/category/activation-functions">activation functions</a><br>
<a href="https://paperswithcode.com/methods/category/affinity-functions">affinity functions</a><br>
<a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">attention mechanisms (1)</a><br>
<a href="https://paperswithcode.com/methods/category/attention-modules">attention modules</a><br>
<a href="https://paperswithcode.com/methods/category/non-parametric-classifiers">classifiers (non-parametric)</a><br>
<a href="https://paperswithcode.com/methods/category/discriminators">discriminators</a><br>
<a href="https://paperswithcode.com/methods/category/feedforward-networks">feedforward nets</a><br>
<a href="https://paperswithcode.com/methods/category/fine-tuning">fine tuning</a><br>
<a href="https://paperswithcode.com/methods/category/approximate-inference">inference (approximate)</a><br>
<a href="https://paperswithcode.com/methods/category/initialization">initialization</a><br>
<a href="https://paperswithcode.com/methods/category/adversarial-training">learning/training (adversarial)</a><br>
<a href="https://paperswithcode.com/methods/category/self-supervised-learning">learning (self-supervised)</a><br>
<a href="https://paperswithcode.com/methods/category/semi-supervised-learning-methods">learning (semi-supervised)</a><br>
<a href="https://paperswithcode.com/methods/category/generalized-linear-models">linear models</a><br>
<a href="https://paperswithcode.com/methods/category/loss-functions">loss functions</a><br>
<a href="https://paperswithcode.com/methods/category/working-memory-models">(working) memory models</a><br>
<a href="https://paperswithcode.com/methods/category/miscellaneous-components">miscellaneous</a><br>
<a href="https://paperswithcode.com/methods/category/neural-architecture-search">neural architecture search</a><br>
<a href="https://paperswithcode.com/methods/category/normalization">normalization</a><br>
<a href="https://paperswithcode.com/methods/category/optimization">optimization</a><br>
<a href="https://paperswithcode.com/methods/category/large-batch-optimization">optimization (large batch)</a><br>
<a href="https://paperswithcode.com/methods/category/stochastic-optimization">optimization (stochastic)</a><br>
<a href="https://paperswithcode.com/methods/category/output-functions">output functions</a><br>
<a href="https://paperswithcode.com/methods/category/parameter-norm-penalties">parameter norm penalties</a><br>
<a href="https://paperswithcode.com/methods/category/parameter-sharing">parameter sharing</a><br>
<a href="https://paperswithcode.com/methods/category/structured-prediction">prediction (structured)</a><br>
<a href="https://paperswithcode.com/methods/category/non-parametric-regression">regression (non-parametric)</a><br>
<a href="https://paperswithcode.com/methods/category/regularization">regularization</a><br>
<a href="https://paperswithcode.com/methods/category/latent-variable-sampling">sampling (latent variables)</a><br>
<a href="https://paperswithcode.com/methods/category/prioritized-sampling">sampling (prioritized)</a><br>
<a href="https://paperswithcode.com/methods/category/skip-connections">skip connections</a><br>
<a href="https://paperswithcode.com/methods/category/skip-connection-blocks">skip connection blocks</a><br>
-->